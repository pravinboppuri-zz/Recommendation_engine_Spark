{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import FloatType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "conf = SparkConf().setAppName(\"app_collaborative\")\n",
    "sc = SparkContext(conf=conf)\n",
    "#sc.setCheckpointDir('checkpoint/')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "USER_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def howFarAreWe(model, against, sizeAgainst):\n",
    "  # Ignore the rating column  \n",
    "  againstNoRatings = against.map(lambda x: (int(x[0]), int(x[1])) )\n",
    "\n",
    "  # Keep the rating to compare against\n",
    "  againstWiRatings = against.map(lambda x: ((int(x[0]),int(x[1])), int(x[2])) )\n",
    "\n",
    "  # Make a prediction and map it for later comparison\n",
    "  # The map has to be ((user,product), rating) not ((product,user), rating)\n",
    "  predictions = model.predictAll(againstNoRatings).map(lambda p: ( (p[0],p[1]), p[2]) )\n",
    "\n",
    "  # Returns the pairs (prediction, rating)\n",
    "  predictionsAndRatings = predictions.join(againstWiRatings).values()\n",
    "\n",
    "  # Returns the variance\n",
    "  return sqrt(predictionsAndRatings.map(lambda s: (s[0] - s[1]) ** 2).reduce(add) / float(sizeAgainst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='/home/bella/Downloads/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userid: string (nullable = true)\n",
      " |-- jobid: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "rddUserRatings = df.filter(df.userid == 0).rdd\n",
    "print(rddUserRatings.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 724, validation: 234, test: 228\n"
     ]
    }
   ],
   "source": [
    "# Split the data in 3 different sets : training, validating, testing\n",
    "# 60% 20% 20%\n",
    "rddRates = df.rdd\n",
    "rddTraining, rddValidating, rddTesting = rddRates.randomSplit([6,2,2])\n",
    "\n",
    "#Add user ratings in the training model\n",
    "rddTraining.union(rddUserRatings)\n",
    "nbValidating = rddValidating.count()\n",
    "nbTesting    = rddTesting.count()\n",
    "\n",
    "print(\"Training: %d, validation: %d, test: %d\" % (rddTraining.count(), nbValidating, rddTesting.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks  = [5,10,15,20]\n",
    "reguls = [0.1, 1,10]\n",
    "iters  = [5,10,20]\n",
    "\n",
    "finalModel = None\n",
    "finalRank  = 0\n",
    "finalRegul = float(0)\n",
    "finalIter  = -1\n",
    "finalDist   = float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best so far:1.227620\n",
      "Best so far:1.225185\n",
      "Best so far:1.166659\n",
      "Rank 5\n",
      "Regul 0.100000\n",
      "Iter 20\n",
      "Dist 1.166659\n"
     ]
    }
   ],
   "source": [
    "for cRank, cRegul, cIter in itertools.product(ranks, reguls, iters):\n",
    "\n",
    "  model = ALS.train(rddTraining, cRank, cIter, float(cRegul))\n",
    "  dist = howFarAreWe(model, rddValidating, nbValidating)\n",
    "  if dist < finalDist:\n",
    "    print(\"Best so far:%f\" % dist)\n",
    "    finalModel = model\n",
    "    finalRank  = cRank\n",
    "    finalRegul = cRegul\n",
    "    finalIter  = cIter\n",
    "    finalDist  = dist\n",
    "#[END train_model]\n",
    "\n",
    "print(\"Rank %i\" % finalRank) \n",
    "print(\"Regul %f\" % finalRegul) \n",
    "print(\"Iter %i\" % finalIter)  \n",
    "print(\"Dist %f\" % finalDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}